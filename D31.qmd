---
title: "D31"
format: html
editor: visual
---

## Open Music Europe D3.1 subtask

We will perform some retroactive survey harmonisation tasks on a selection of KULT surveys for the period of 2007-2023 in order to prepare for an ex ante harmonisation of these important data points of the Slovak cultural statistics going forward from 2024.

Reprex, on the basis of our MoU, but with a separate legal agreement received access to the microdata and the codebooks and variable maps concerning these surveys. The microdata will not get into the possession of the Consortium, but Sinus will supervise the entire oversight of the reproducible workflow. The eventual new statistical products will go through automated unit tests, and tests by Sinus, IKP and the Statistical Office of SR. This workflow is in line with the evolving statistical practice of working with privately held administrative data. The microdata is placed in 15 tabular CSV files (one per survey and year), with a pdf variable and codelist in Slovak only. We want to add these microdata into a database that can create a new statistical datacube from the microdata so that they integrate easily with data from other surveys. Variable labelling and naming. Concept mapping: identifying key concepts relevant for WP1, WP2, WP3, such as employment concepts, and add global (standard) identifiers for concepts such as "employment" or "artistic employment" for future harmonisation with other data sources and ex ante harmonisation with new surveys. A namespace for observation (statistical) units with local, national Slovak and global identifiers. A codelists for all categorical variables, whenever applicable, using the SDMX or ESTAT standardised codelists. Crosswalking every individual dataset with the correct variable names, labels, and variable codes. Adding the global identifiers and the missing time dimensions to the datasets. Joining each survey programs data (for example, all KULT 05-01 survey data) into a single, integrated dataset. Recreate selected statistics (averages, sums) and unit test them against published values of the SOSR. Create forecasts of the data for up to three years. Create visualisation and policy analysis tables.

Overall aim We would like to improve the coverage, usability and punctuality of Slovak cultural statistics, particularly in the field of music and to some extent, connecting domains that are currently served by the same data sources (mainly audiovisual statistics, which by some definitions overlap with music.) The primary data source of these statistics is the KULT survey program. The KULT surveys are designed to produce cross-sectional data that is comparable over short periods of time. They are not designed to be used for the creation of more valuable, longer longitudinal datasets. The KULT microdata, like most statistical data sources, needs to be fully utilised. Survey data cannot be collected ex post, and statistical programs "over-collect" and do not necessarily utilise every information content in the microdata, because that part of the statistical production is subject to a partly different cost-value analysis. This means that often we may fill data gaps from the secondary processing of existing data sources. Whenever this is not the case, it is a requirement to fill data gaps in a way that maintains the consistency of the both the statistical product and the statistical workflows in general, for example, collection. The limitation of subject burden (an orchestra cannot be expected to fill out many questionnaires with precision) means that the questionnaire design should be streamlined. Our task is to create an ex post or retrospective harmonisation of the KULT data to find ways to improve, enrich or reuse the existing historical data and to describe new data to be collected going forward potentially. This is a cross-cutting task between tasks 1.1, 2.1, 3.1, 4.1 and 5.1, i.e. a Taskstream 1 task that sets requirements for data to be collected and collection tools to be made available in the Taskstream 2. This means that WP1 and WP2 are involved in defining and prioritising the data gaps to be filled and the concepts of music economy, music diversity and music circulation that require data collection. In 3.1 and 4.1 we use retrospective data harmonisation to see if we can "squeeze out" more data from existing but underutilised datasets or if we need to experiment with new data to be collected with survey methods in Taskstream 2. Another important expected result is to validate data gaps from a different point of view: sometimes, a data gap is not identified because the data appears to be present, but the quality of the data is so low that stakeholders do not use it. This appears to be the case in several countries with concert attendance, where the concert attendance measured by statistical offices seems to differ greatly from the administrative data of collective management societies that license the music and often know about every ticket sold.

Our retrospective harmonisation can pinpoint data that has a low data quality. In subsequent tasks, we can offer various methods to improve such data. Our work is an essential methodological work required for the modernisation of music and cultural statistics, and we are planning to present it to the working group of the 27 member states's cultural statisticians in 2024 for peer review feedback and to review pan-European scalability. To our knowledge, our work with the KULT statistics is the most comprehensive data review in music and perhaps cultural statistics in Europe conducted outside the statistical offices. Some elements of these workflows are supported by the current version of the retroharmonise R package, and some new functionality needs to be added to it or improved. This is a task for 4.1 that we organise with three priorities: to be ready in 2023, to be ready by January 2024, and later. Some elements of the workflow, particularly adding global identifiers for future linking are new functionalities that will require new programming tasks for 4.1. We aim to finalise a first batch of retrospective revisions of select KULT surveys and datasets by early January 2024 and open it for a stakeholder peer review for the Hudobn√© centrum (data steward for music KULT), the Audiovisual Fund (data steward for audiovisual aspects), the National Library (covered by KULT, but also an authority control agency for the Slovak namespace), Wikipedia Slovensko, SOZA, Hudobny Fond, other stakeholders, and the SOSR. We have already started discussions with the DG EAC Cultural Policy Unit \[check the name!\] and the ESCO unit of \[DG ?\] to start checking the feasibility of scaling up our work to more member states. \[How about Rosina?\] Based on this consultation, we will make recommendations for ex ante harmonisation and future modifications in the way KULT is produced, particularly with questionnaire modifications, improving the representativeness of the target population, and offering links to match the data from KULT with other data sources. Because the changing cycle of KULT is two years in SK, we must be ready with our first retrospective review by 7 January 2024.

Methodology Survey harmonisation is metadata-heavy work; it is the metadata that allows for a precise and machine-supported harmonisation of the data. To ensure that the metadata allows us to successfully integrate KULT data from 2007 and 2022, we must ensure that the necessary, machine-actionable metadata is added to each dataset in the time period 2007-2023 in a way that is useful for future data collection tasks. WP4 and WP4 uses the eXtreme Design (XD) methodology extended with some UX design concepts for this task. This methodology enables us to use the expertise of researchers in WP1-2-3 and external stakeholders.

### Concept and variable mapping

The KULT survey documentation contains Slovak language descriptions of the variables in a narrative format, often together with coding information. We create a machine-readable and programmatically easy-to-handle variable name for each variable, therefore creating the mapping 168 "total_own_turnover", and whenever necessary, we create a new auxiliary variable. The auxiliary variables are usually attribute variables that were constant in each dataset, and they were not explicitly coded, such as the time reference for the year 2007 or the unit of measure reference for SKK or EUR. For programmatic use, we will use only ASCII characters to avoid spaces and characters with special meanings in various programming languages. Variable naming is usually following either the camelCase or the snake_case convention. Because in the tidy use of R the snakecase is recommended, we will create snake case variable names.

We will translate each variable label to English and add the 1-2 most relevant concepts for all variables to make the variable descriptions machine-actionable. Concept mapping requires a good command of the domain and statistically controlled vocabularies. Our initial concept mapping will go through two layers of peer review: Hudobne Centrum, the data steward organisation of the KULT surveys, will review our mapping first from a musical point of view, and we will ask the SOSR for a statistical revision.

As an end result, we will be able to bring each dataset column to a machine-actionable format that already offers itself for ex ante harmonisation with further surveys and data sources. For convenience, we will make the new joined datasets available in CSV, Excel, and SPSS formats. This will be a manual lookup job supervised by James and Daniel for SSSA or EUBA. This work cannot be meaningfully automated because we are adding human control and agency to the supervision of automated and potentially AI-driven processes.

There is no way to avoid this task because it requires domain-specific and statistical knowledge. The WP1 and WP2 teams must identify a standard codelist for each concept, such as SDMX's SEX, which provides the M=male, F=female, \_Z=other coding for sex and gender. Whenever possible, we must use an existing and globally accepted code list. In WP4.1 we should amend the statcodelists package so that these codelist are included in native R format in the data package \[this is a must in December, at least to a few select variables that we will showcase in January; as the KULT mainly uses numbers and numeric data, it is expected that the number of codelists will be limited and already in statcodelists; for example, currency codes, time interval and time period codes\]

### Namespaces and observational unit identification

Namespaces and authority files The KULT surveys are enterprise surveys by nature, although they relate to social or cultural enterprises, which often do not have an enterprise form or even a legal personality. The statistical (observational) units are orchestras, festivals, libraries, and similar cultural institutions. When we want to work with longitudinal or panel data, we have to ensure that \[fictional example: New Kosice Orchestra and Contemporary Kosice Orchestra is kept as one observational unit if they only went through name changes, and if the identify fundamentally changed, they are kept separately. \]

Namespaces (in the statistical and data science practice) and authority files (in information and library science) contain exactly this information: they describe the 'concept' of an observational unit, connect various name variations that were in use, including possible nicknames, and defines the validity of the concept in time. For example, no data must be recorded about the \[New Kosice Orchestra before its founding year of 2007\]. Eventually, we can see this task as a "concept matching" for statistical units or observation units and not for variables. This kind of identity and concept matching allows the add data about the observation units from various administrative and survey data sources and provides a data harmonisation on the level of observations (rows).

An important benefit of this observation unit identification is that microdata can be resampled or improved when the original coverage of the target population varies across years. For longitudinal or panel data, it requires judgment how long we can observe the same target population.

The use of PIDs for observational units allows data improvements and increases the longevity of datasets. In laymen's terms, we can say that we can increase the historical time series coverage meaningfully with this harmonisation; we can add more history to data than normal statistical production would give. For example, if there were greatly different orchestras in 2007 and 2022, we can create a "stable group of orchestras" and compare them separately and meaningfully over 15 years. The KULT program -- like all the enterprise surveys of Slovakia -- use the ICO identifier for entities as a global ID within the administrative records of the Slovak Republic.

The ICO identifier is a public identifier for organisations, but some organisations are not eligible to have one. For example, if a radio station or a university has a choir as an internal unit, it may not be eligible for an ICO. The datasets in this case, use the organisation's name as an ID, which is acceptable in a single year but is very problematic over 15 years. We see in the KULT files that the lack of a strict namespace or authority file creates identification problems; some observational units appear over different names; sometimes, the name variations appear to be simple data entry errors (spelling mistakes.)

The best practice of observation identification is the use of a persistent global identifier (PID). The use of the PID allows us to connect data and knowledge about the *fictional* *New Kosice Orchestra* across various data sources. Furthermore, the use of PIDs will enable Open Music Europe to scale up our data harmonisation efforts beyond Slovakia, for example, benchmarking Slovak data against Bulgarian data. The ICO code is not in use outside the SR; therefore, it is far better to identify the *New Kosice Orchestra* with a global ISO-standard identifier, for example, with an ISNI code.

Global namespaces and authority files are, by design, machine-actionable; they can be read by humans, activating a html representation for human browsing and an RDF serialisation for data applications. We will encourage the use of at least two PIDs for each observational unit, following the best practices that will be elaborated in greater detail in 5.1.

-   We will use either ISNI or VIAF identifiers for natural and legal persons. The use of ISNI is paid, and the organisation itself must initiate it. The use of VIAF is free because it is a public service of the Slovak National Library, but it goes through a curation process. For our purposes, they are equally good and they can be used interchangeably.

-   We will also use a QID for data coordination. The QID is a globally unique identifier in Wikidata and Dbpedia. They are used in the statistical, research and cultural heritage domains as temporary or necessary global IDs when the use of an authority file is not possible or takes a long time (for example, we have to encourage each orchestra to obtain its own ISNI number.)

The use of Wikidata is getting more and more common among knowledge organisations and even EU organisations. Originally developed as a reconciliation tool for Wikipedia, Europeana already recognised its value for pan-European data harmonisation in 2015. Since that, it had been used as a decentralised, curated, shared authority control system in several European countries. We think that VIAF is the most suitable authority control, but the flexibility and functionality of Wikidata makes it a worthy parallel system in itself [@bianchini_beyond_2021; @van_veen_wikidata_2019; @rossenova_wikidata_2022] We reached out to the Wikimedia Foundation and *WMSK*, former official legal name *Wikimedia Slovensk√° republika* to not only use their open source product, i.e, Wikibase for authority control reconciliation, but as a tool to push our knowledge and our namespace to the Wikidata. [@fagerving_wikidata_2023]

### Crosswalking

Crosswalking the information means that the current metadata of the KULT datasets is augmented or changed to the new metadata: the programmatically useable variable names, the multi-language variable labels, the units of measure, and the global IDs of observational units. This will be an entirely reproducible and machine supported task.\
The current (background) version of retrohamonise has a crosswalk function, but it needs to be extended or potentially completely rewritten, mainly because it does not handle observation (row) data, only variable data (columns). Crosswalking is a purely technical metadata task that should be made with statistical software that provides extensive built-in unit testing; human supervisors should review the data after reviewing unit-test logs. Crosswalking on this scale means correct labelling of several hundred thousand cells, which cannot be performed or supervised meaningfully in a manual workflow, for example, in a spreadsheet editor. New task for WP4.1 In the development plan created by Daniel, the 0.4.3 of retroharmonise has a basic crosswalking function that should be generalised enough to be part of a new crosswalk package at one point. Crosswalking is a very general data/metadata process. It is useful for survey harmonisation and can also be used to match survey data with administrative data sources (for example, music royalty accounts data with musician survey data.) Crosswalking should use DDI-compliant crosswalk tables, DDI/SDMX codelists.

Task 1: Small improvements or system requirements setting for the current crosswalk functions in December 2023.

Task 2: Extending the usability of our crosswalking functions in 2024 in retroharmonise.

Task 3: dividing the retroharmonise package to at least two packages, because the new crosswalk package can support non-survey data, or survey/non-survey reconciliation. Creation of statistics The creation of statistics means that we will first re-create existing Slovak cultural statistics, then we will create new ones from the existing data, and then we suggest the creation of new statistics with new, ex ante harmonised datasets. Most of the KULT data relates to numbers, where the meaningful statistics are average, weighted average, mean, and sum (and potentially some other distributional data such as percentile or quantile statistics.) We already know some immaterial data errors in our files (some summaries for example, have a small deviation from the officially released statistics.) We will use these deviations to create unit tests for our production. The KULT microdata, like most statistical microdata, is not particularly useful for an end-user and cannot be published for an end-user -- we even limit the number of people who can see it in its entirety to connect it with personal identifiers. For business and policy uses, statistical indicators are far more useful, and most end-users do not have the skills and infrastructure to create from microdata reliably the statistical indicators. We will fill the data gaps on a statistical indicator level. Our expectation that the critical revision of the microdata will allow us to fill some data gaps with new statistics (for example, average budgets of orchestras, etc.) that had not been available earlier. This is a logical first step before going forward with our Taskstream 2: data that already exists but it is not used for the creation of statistical indicators should not be recollected, because it would be a waste of resources both on the collector's side and on statistical subject's side (filling out the same questionnaire items twice.)

*WP4 task We should create an R pipeline that creates all statistics needed by the users (starting with mean, median, sum) supported with backcasting, forecasting and 1-2 imputation forms. Ideally, this should be packaged for documentation and usability purposes, but most functions should reuse the forecast, tsibble, and purrr packages. This will feed the D5.1 OMO with actual new datasets on scale.*

### Imputation, backcasts and forecasts

It is a good statistical practice to test the longitudinal stability of the data, and to provide forward-looking estimates, backward-looking backkasting, or interpolation of missing data points in time series when the dynamics of the statistics develops in a predictable way. While such calculations play an important role in quality control, they may have individual user values. For budgeting, business or policy planning, it is usually a good starting point to have a "base case", for example, a reliable forecast of concert visitors, if concert visits appear to follow a regular pattern. Such forecasts (and, if needed, for example, in sustainability measurements, backcasts) are not always trivial tasks and require statistical judgement. The lack of imputation (for example, a data point is missing in the year 2015) can particularly create difficulties for the end users in using econometric models or even visualisations. It is a normal practice regulated by SDMX and the European quality control of statistics to produce "official" forecasts, imputations and backcasts by flagging such data points clearly as not actual but forecasted or imputed data.

*WP4 task We should create an R pipeline that creates all statistics needed by the users (starting with mean, median, sum) supported with backcasting, forecasting and 1-2 imputation forms. Ideally, this should be packaged for documentation and usability purposes, but most of the functions should be reusing the forecast, tsibble, purrr packages. This will feed the D5.1 OMO with actual new datasets on scale.*

### Unit of measure

The conversion among units of measure, for example, grams, tons or kilograms of CO2 emissions, or euros, thousand and million euros, are a source of many data handling errors. The conversion of among Slovak koruna, dollars (OECD and World Bank datasets) and euros (Euostat and SOSR data) is often not even trivial, as it requires judgement if year-end, year-begin, midpoint or average currency rates should be used for correct and consistent currency unit translations.

We will produce our datasets using Eurostat standards for units of measures and units of measure decimals. We will create statistics in euros and dollars, with conversion for the last SKK years to both currencies. WP4 task: Unit of measure conversions are expected, but perhaps they are limited to alignment of decimals and the SKK/EUR/USD conversion.
